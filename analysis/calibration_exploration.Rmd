---
  title: "Exploring parameter sensitivity and calibration"
author: "Pepa Aran"
date: "2023-04-27"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exploring parameter sensitivity and calibration}
%\VignetteEngine{knitr::rmarkdown}
%\usepackage[utf8]{inputenc}
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(rsofun)
library(dplyr)
library(ggplot2)
library(tidyr)
library(sensitivity)
library(BayesianTools)
```

Parameter calibration can have a big impact on our modeling effort and use
big computational resources. Hence, it is worth our time to explore which parameters
should actually be calibrated (because of they impact the simulations greatly) 
and to examine if the calibration routines behave as expected. 
This vignette explains how to perform a simple
parameter sensitivity analysis for the P-model and how to interpret the outputs of the
calibration routines using the `BayesianTools` package.

# Morris sensitivity analysis

The Morris method for global sensitivity analysis allows to explore which parameters have the
biggest influence on the model fit. In this example, we will quantify how different
values of the calibratable parameters (`kphio`, `soilm_par_a`, `soilm_par_b`,
`tau_acclim_tempstress` and `par_shape_tempstress`) lead to more variability
in the match between GPP predicted by the P-model and GPP observations. It would
be wise to repeat this exercise for various targets because they may be simulated
by equations in the P-model that involve different model parameters, 
later we do it for Vcmax25.

If the P-model
has very low sensitivity to a certain parameter, calibrating this parameter will
not improve the model substantially. But if it's very sensitive to another parameter,
calibrating this second parameter could improve the P-model fit greatly. We should
spend our computational resources on calibrating the parameters to which the
model is most sensitive.

## Sensitivity analysis with GPP as target

First of all, let's define a function which measures the agreement between
GPP predictions from the P-model and GPP observations, for a set of values of
the calibratable parameters. It computes the log-likelihood of 
the GPP predictions, given the observed GPP and its uncertainty.
We want to see how sensitive this function is to changes in the parameter values.

```{r}
# Define log-likelihood function for GPP
ll_pmodel_gpp <- function(
    par_v               # a vector of calibratable parameters including errors
){
  # define targets
  targets <- c('gpp', 'vcmax25')
  
  # get parameters into correct format
  par_list <- as.list(par_v[-6])  |>        # remove error parameter
    setNames(c(
      "kphio",
      "soilm_par_a",
      "soilm_par_b",
      "tau_acclim_tempstress",
      "par_shape_tempstress"))
  
  # simulate the model for the parameter list
  sim.df <- rsofun::runread_pmodel_f(
    drivers = rsofun::p_model_drivers,             # example forcing data
    par = par_list
  ) 
  
  # clean output
  sim.df <- sim.df |>
    dplyr::select(sitename, data) |>
    tidyr::unnest(data) |>                 
    dplyr::rename(gpp_mod = gpp)           # rename target column
  
  # get observations
  obs.df <- rsofun::p_model_validation |>  # example observed data
    dplyr::select(sitename, data) |>
    tidyr::unnest(data)
  
  # join observations and simulation output
  sim.df <- sim.df |>
    dplyr::left_join(
      obs.df, by = c("sitename", "date")
    ) |>
    tidyr::drop_na()     # remove NAs
  
  # calculate normal log-likelihood
  ll <- sum(
    stats::dnorm(
      sim.df[['gpp_mod']],                 # simulated GPP
      mean = sim.df[['gpp']],              # observed GPP
      sd = par_v[6],                       # error term
      log = TRUE
    ))  
  
  if(is.nan(ll) | is.na(ll) | ll == 0 ){ ll <- - Inf}
  return(ll)
}

# Compute log-likelihood for a given set of parameters
ll_pmodel_gpp( par_v = c(
  kphio           = 0.09423773,
  soilm_par_a     = 0.33349283,
  soilm_par_b     = 1.45602286,
  tau_acclim_tempstress = 10,
  par_shape_tempstress  = 0.0,
  error_gpp       = 0.9          # value from previous simulations
))
```

Next, let's define the parameter space by their lower and upper bounds. Some
parameters are constrained by their physical interpretation (e.g. `kphio > 0`)
and it's also necessary to provide a bounded parameter space for the Morris
method to sample the parameter space. 
```{r}
# best parameter values (from previous literature)
par_cal_best <- c(
    kphio           = 0.09423773,
    soilm_par_a     = 0.33349283,
    soilm_par_b     = 1.45602286,
    tau_acclim_tempstress = 10,
    par_shape_tempstress  = 0.0,
    error_gpp       = 0.9
  )

# lower bound
par_cal_min <- c(
    kphio           = 0.01,
    soilm_par_a     = 0,
    soilm_par_b     = 1,
    tau_acclim_tempstress = 0,
    par_shape_tempstress  = -0.5,
    error_gpp       = 0.01
  )

# upper bound
par_cal_max <- c(
    kphio           = 5,
    soilm_par_a     = 1,
    soilm_par_b     = 3,
    tau_acclim_tempstress = 20,
    par_shape_tempstress  = 0.5,
    error_gpp       = 2
  )
```

We use the `morris()` function from the `{sensitivity}` package to perform
the sensitivity analysis. As a target function, we will use the posterior
density (log-likelihood) of the parameters given the GPP data which we obtain
via the function `BayesianTools::createBayesianSetup()`. Note that, because of using
a uniform prior, the posterior distribution is proportional to the GPP 
log-likelihood (defined previously) 
wherever the parameter values are feasible and zero outside of the parameter ranges.
```{r}
morris_setup <- BayesianTools::createBayesianSetup(
  likelihood = ll_pmodel_gpp,
  prior = BayesianTools::createUniformPrior(par_cal_min, par_cal_max, par_cal_best),
  names = names(par_cal_best)
)
```

In the following chunk, we run the Morris sensitivity analysis, using a grid with
r=500 values for each parameter and a one-at-a-time design. Running the
sensitivity analysis may take a few minutes, even for this small example dataset,
and is still computationally cheaper than running the parameter calibration.
```{r}
set.seed(432)
morrisOut <- sensitivity::morris(
  model = morris_setup$posterior$density,
  factors = names(par_cal_best), 
  r = 500, 
  design = list(type = "oat", levels = 20, grid.jump = 3), 
  binf = par_cal_min, 
  bsup = par_cal_max, 
  scale = TRUE)
```

The analysis evaluates the variability of the target function, i.e. the 
log-likelihood, for several points across the parameter space. 
Statistics $\mu *$ and $\sigma$ measure the average of the absolute
differences 
```{r}
# summarise the moris output
morrisOut.df <- data.frame(
  parameter = names(par_cal_best),
  mu.star = apply(abs(morrisOut$ee), 2, mean, na.rm = T),
  sigma = apply(morrisOut$ee, 2, sd, na.rm = T)
) %>%
  arrange( mu.star )
```

Let's observe the results:
```{r}
morrisOut.df |>
  tidyr::gather(variable, value, -parameter) |>
  ggplot(aes(
    reorder(parameter, value),
    value, 
    fill = variable),
    color = NA) +
  geom_bar(position = position_dodge(), stat = 'identity') +
  scale_fill_brewer("", labels = c('mu.star' = expression(mu * "*"),
                                   'sigma' = expression(sigma)),
                    palette = "Dark2") +
  theme_classic() +
  theme(
    axis.text = element_text(size = 6),
    axis.title = element_blank(),
    legend.position = c(0.05, 0.95), legend.justification = c(0.05, 0.95)
  )

```

## Sensitivity analysis with Vcmax25 as target
```{r}
# Define log-likelihood function for Vcmax25
ll_pmodel_vcmax25 <- function(
    par_v               # a vector of calibratable parameters including errors
){
  
  # get parameters into correct format
  par_list <- as.list(par_v[-6]) # remove error parameter
  names(par_list) <- c(
    "kphio",
    "soilm_par_a",
    "soilm_par_b",
    "tau_acclim_tempstress",
    "par_shape_tempstress"
  )
  
  # simulate the model for the parameter list
  sim.df <- rsofun::runread_pmodel_f(
    drivers = rsofun::p_model_drivers_vcmax25 |>
      dplyr::slice(1:10),             # example forcing data, only 10 sites
    par = par_list
  )
  
  # clean output
  sim.df <- sim.df |>
    dplyr::select(sitename, data) |>
    tidyr::unnest(data) |>                 
    dplyr::rename(vcmax25_mod = vcmax25)           # rename target column
  
  # get observations
  obs.df <- rsofun::p_model_validation_vcmax25 |>
    dplyr::slice(1:10) |>                # example observed data, olnly 10 sites
    dplyr::select(sitename, data) |>
    tidyr::unnest(data)
  
  # join observations and simulation output
  sim.df <- sim.df |>
    dplyr::left_join(
      obs.df, by = c("sitename")
    ) |>
    tidyr::drop_na()     # remove NAs
  
  # calculate normal log-likelihood
  ll <- sum(
    stats::dnorm(
      sim.df[['vcmax25_mod']],                 # simulated GPP
      mean = sim.df[['vcmax25']],              # observed GPP
      sd = par_v[6],                           # error term
      log = TRUE
    ))  
  
  if(is.nan(ll) | is.na(ll) | ll == 0 ){ ll <- - Inf}
  return(ll)
}

ll_pmodel_vcmax25( par_v = c(
  kphio           = 0.09423773,
  soilm_par_a     = 0.33349283,
  soilm_par_b     = 1.45602286,
  tau_acclim_tempstress = 10,
  par_shape_tempstress  = 0.0,
  error_vcmax25       = 0.001          # value from previous simulations
))

# best parameter values (from previous literature)
par_cal_best_vcmax25 <- c(
    kphio           = 0.09423773,
    soilm_par_a     = 0.33349283,
    soilm_par_b     = 1.45602286,
    tau_acclim_tempstress = 10,
    par_shape_tempstress  = 0.0,
    error_vcmax25       = 0.001
  )

# lower bound
par_cal_min_vcmax25 <- c(
    kphio           = 0,
    soilm_par_a     = 0,
    soilm_par_b     = 1,
    tau_acclim_tempstress = 0,
    par_shape_tempstress  = -0.5,
    error_vcmax25       = 0.00001
  )

# upper bound
par_cal_max_vcmax25 <- c(
    kphio           = 5,
    soilm_par_a     = 1,
    soilm_par_b     = 3,
    tau_acclim_tempstress = 20,
    par_shape_tempstress  = 0.5,
    error_vcmax25       = 0.2
  )

# define priors
morris_setup_vcmax25 <- BayesianTools::createBayesianSetup(
  likelihood = ll_pmodel_vcmax25,
  prior = BayesianTools::createUniformPrior(
    par_cal_min_vcmax25, par_cal_max_vcmax25, par_cal_best_vcmax25
    ),
  names = names(par_cal_best)
)
```

The setup is defined, now let's perform the sensitivity analysis:
```{r}
set.seed(432)
morrisOutvcmax25 <- sensitivity::morris(
  model = morris_setup_vcmax25$posterior$density,
  factors = names(par_cal_best_vcmax25), 
  r = 500, 
  design = list(type = "oat", levels = 20, grid.jump = 3), 
  binf = par_cal_min_vcmax25, 
  bsup = par_cal_max_vcmax25, 
  scale = TRUE)
```

Then we can observe the results:
```{r}
# summarise the moris output
morrisOutvcmax25.df <- data.frame(
  parameter = names(par_cal_best_vcmax25),
  mu.star = apply(abs(morrisOutvcmax25$ee), 2, mean, na.rm = T),
  sigma = apply(morrisOutvcmax25$ee, 2, sd, na.rm = T)
) %>%
  arrange( mu.star )

# plot the output
morrisOutvcmax25.df |>
  tidyr::gather(variable, value, -parameter) |>
  ggplot(aes(
    reorder(parameter, value),
    value, 
    fill = variable),
    color = NA) +
  geom_bar(position = position_dodge(), stat = 'identity') +
  scale_fill_brewer("", labels = c('mu.star' = expression(mu * "*"),
                                   'sigma' = expression(sigma)),
                    palette = "Dark2") +
  theme_classic() +
  theme(
    axis.text = element_text(size = 6),
    axis.title = element_blank(),
    legend.position = c(0.05, 0.95), legend.justification = c(0.05, 0.95)
  )
```

# Interpretation of Bayesian calibration routine - GPP and Vcmax25

It is always important to check the convergence of the MCMC algorithm used for the Bayesian calibration. We can plot the MCMC chains, returned as part of the `calib_sofun()` output, and see that they converge to a similar value and become progressively more stable. We can also see the probability density of the sampled parameter values. 

```{r}
# Define general log-likelihood cost function
cost_likelihood_pmodel <- function(
    par,   # kphio & error terms for each target
    obs,
    drivers,
    targets
){
  # predefine variables for CRAN check compliance
  sitename <- data <- NULL
  
  ## execute model for this parameter set
  params_modl <- list(
    kphio           = par[1],
    soilm_par_a     = 0.333,
    soilm_par_b     = 1.45,
    tau_acclim_tempstress = 10,
    par_shape_tempstress = 0)
              
  # Check if there are as many error terms as targets
  if(length(targets) != (length(par) - 1)){
    error('There must be a calibratable error parameter per target variable,
          and in the same order.')
  }
  
  # run the model
  df <- runread_pmodel_f(
    drivers,
    par = params_modl,
    makecheck = TRUE,
    parallel = FALSE      # shall this be an input parameter?
  )
  
  # clean model output and unnest
  df <- df |>
    dplyr::rowwise() |>
    dplyr::summarise(
      cbind(sitename, data[, c('date', targets)]) |>
        setNames(c('sitename', 'date', paste0(targets, '_mod')))
    )
  
  # unnest validation data, separated into traits and fluxes
  is_flux <- apply(obs, 1, function(x){ 'date' %in% colnames(x$data)})
  
  # left join with flux observations
  if(sum(is_flux) > 0){
    df <- dplyr::left_join(df, 
                           obs[is_flux, ] |>
                             dplyr::select(sitename, data) |>
                             tidyr::unnest(data), 
                           by = c('sitename', 'date'))
  }
  # left join with trait observations
  if(sum(!is_flux) > 0){
    df <- dplyr::left_join(df, 
                           obs[!is_flux, ] |>
                             dplyr::select(sitename, data) |>
                             tidyr::unnest(data), 
                           by = c('sitename'))
  }

  # loop over targets
  ll <- sapply(seq(length(targets)), function(i){
    target <- targets[i]
    # get observations and predicted target values, without NA            
    df <- df[, c(paste0(target, '_mod'), target)] |>
      tidyr::drop_na()
    
    # calculate normal log-likelihood
    ll <- sum(stats::dnorm(
      df[[paste0(target, '_mod')]],
      mean = df[[target]],
      sd = par[length(par)-length(targets) + i],
      log = TRUE
    ))
  }) |>
    sum()

  # trap boundary conditions
  if(is.nan(ll) | is.na(ll) | ll == 0){ll <- -Inf}
  
  return(ll)
}
```

After we have a cost function for the BRC setup and that works for several 
targets (one at a time or for simultaneous calibration), we can run the calibrations:
```{r}
### Calibration for GPP only
# Define calibration settings
settings_calib_gpp <- list(
  method = "BayesianTools",
  metric = function(par, obs, drivers){
    cost_likelihood_pmodel(par = par, obs = obs, drivers = drivers,
                           targets = c('gpp'))
  },
  control = list(
    sampler = "DEzs",
    settings = list(
      burnin = 500,
      iterations = 1000
    )),
  par = list(
    kphio = list(lower = 0, upper = 0.2, init = 0.05),
    err_gpp = list(lower = 0.1, upper = 2, init = 0.8)
  )
)

# calibrate parameters kphio and err_gpp (the two to which the model is most
# sensitive)
par_calib_gpp <- calib_sofun(
  drivers = p_model_drivers,
  obs = p_model_validation,
  settings = settings_calib_gpp
)
```

```{r}
### Calibration for Vcmax25 only
# Define calibration settings
settings_calib_vcmax25 <- list(
  method = "BayesianTools",
  metric = function(par, obs, drivers){
    cost_likelihood_pmodel(par = par, obs = obs, drivers = drivers,
                           targets = c('vcmax25'))
  },
  control = list(
    sampler = "DEzs",
    settings = list(
      burnin = 500,
      iterations = 1000
    )),
  par = list(
    kphio = list(lower = 0, upper = 0.2, init = 0.05),
    err_vcmax25 = list(lower = 0.000001, upper = 0.2, init = 0.01)
  )
)

# calibrate parameters kphio and err_gpp (the two to which the model is most
# sensitive)
par_calib_vcmax25 <- calib_sofun(
  drivers = p_model_drivers_vcmax25 |>
    dplyr::slice(1:10),
  obs = p_model_validation_vcmax25 |>
    dplyr::slice(1:10),
  settings = settings_calib_vcmax25
)
```

```{r}
### Simultaneous calibration
# Define calibration settings
settings_calib <- list(
  method = "BayesianTools",
  metric = function(par, obs, drivers){
    cost_likelihood_pmodel(par = par, obs = obs, drivers = drivers,
                           targets = c('gpp', 'vcmax25'))
  },
  control = list(
    sampler = "DEzs",
    settings = list(
      burnin = 500,
      iterations = 2000
    )),
  par = list(
    kphio = list(lower = 0, upper = 0.2, init = 0.05),
    err_gpp = list(lower = 0.1, upper = 2, init = 0.8),
    err_vcmax25 = list(lower = 0.000001, upper = 0.2, init = 0.01)
  )
)

# calibrate parameters kphio and err_gpp (the two to which the model is most
# sensitive)
par_calib <- calib_sofun(
  drivers = rbind(p_model_drivers,
                  p_model_drivers_vcmax25 |>
                    dplyr::slice(1:10)),
  obs = rbind(p_model_validation, 
              p_model_validation_vcmax25 |>
                dplyr::slice(1:10)),
  settings = settings_calib
)
```

Okay, after calibrating let's check out the result!
```{r}
params_modl <- list(
    kphio           = 0.0942,
    soilm_par_a     = 0.333,
    soilm_par_b     = 1.45,
    tau_acclim_tempstress = 10,
    par_shape_tempstress = 0)

par(mfrow = c(3,4))

gpp_data <- p_model_validation %>%
  tidyr::unnest(data) |>
  dplyr::slice(1:110)          # get first month of data

site_vcmax <- 43:46                                 # can be changed
vcmax25_data <- p_model_validation_vcmax25 |>
  dplyr::ungroup() |>
  dplyr::slice(site_vcmax) |>        # get 3 first sites
  tidyr::unnest(data)

# Plot GPP calibration ####
params_new <- params_modl
params_new$kphio <- par_calib_gpp$par[1]

ggplot(data = rsofun::runread_pmodel_f(
        p_model_drivers,
        par = params_new
      ) |>
      dplyr::select(sitename, data) |>
      tidyr::unnest(data) |>
          dplyr::slice(1:110)) +
    geom_line(aes(
      date,
      gpp
    ),
    colour = "red",
    alpha = 0.8
  ) +
  geom_errorbar(
    aes(x = date, ymin=gpp - 2*par_calib_gpp$par[2],   # should cover ~ 95% of observations
        ymax=gpp + 2*par_calib_gpp$par[2]), width=.2,
                 position=position_dodge(0.05), 
    color = 'red', alpha = 0.5) +  
  geom_line(
    data = gpp_data,
    aes(
      date,
      gpp
    ),
    alpha = 0.8
  ) +
  geom_errorbar(
    data = validation_data,
    aes(x = date, ymin=gpp - 2*gpp_unc, ymax=gpp + 2*gpp_unc), width=.2,
                 position=position_dodge(0.05)) +

  labs(
    x = "Date",
    y = "GPP"
  )

# Plot Vcmax25 calibration ####
params_new$kphio <- par_calib_vcmax25$par[1]

ggplot(data = rsofun::runread_pmodel_f(
        p_model_drivers_vcmax25 |>
          dplyr::slice(site_vcmax),
        par = params_new
      ) |>
      dplyr::select(sitename, data) |>
      tidyr::unnest(data)) +
    geom_line(aes(
      date,
      vcmax25
    ),
    colour = "red",
    alpha = 0.8
  ) +
  geom_errorbar(
    aes(x = date, ymin=vcmax25 - 2*par_calib_vcmax25$par[2],   # should cover ~ 95% of observations
        ymax=vcmax25 + 2*par_calib_vcmax25$par[2]), width=.2,
                 position=position_dodge(0.05), 
    color = 'red', alpha = 0.5) +  
  geom_hline(
    data = vcmax25_data,
    aes(
      yintercept = vcmax25
    ),
    alpha = 0.8
  ) +
  geom_hline(
    data = vcmax25_data,
    aes(yintercept = vcmax25 - 2*vcmax25_unc),
    alpha = 0.5
    ) +
  geom_hline(
    data = vcmax25_data,
    aes(yintercept = vcmax25 + 2*vcmax25_unc),
    alpha = 0.5) +
  labs(
    x = "Date",
    y = "Vcmax25"
  ) +
  facet_grid(~ sitename)
```

Let's check the convergence of the MCMC chains for all the calibration setups:
```{r}
plot(par_calib_gpp$mod)
plot(par_calib_vcmax25$mod)
plot(par_calib$mod)
```

### Do the calibrated error terms make sense?

Let's take a look at the different sources of error in the model, focusing on GPP as target.
```{r eval = evaluate}
validation_data <- p_model_validation %>%
  tidyr::unnest(data) |>
  dplyr::slice(1:30)          # get first month of data

ggplot() +
  #   geom_line(
  #   data = model_data,
  #   aes(
  #     date,
  #     gpp
  #   ),
  #   colour = "red",
  #   alpha = 0.8
  # ) +
  geom_line(
    data = validation_data,
    aes(
      date,
      gpp
    ),
    alpha = 0.8
  ) +
  geom_errorbar(
    data = validation_data,
    aes(x = date, ymin=gpp - 2*gpp_unc, ymax=gpp + 2*gpp_unc), width=.2,
                 position=position_dodge(0.05)) +
  labs(
    x = "Date",
    y = "GPP"
  )

```

Then, let's see what the average model error is, i.e. the RMSE between the 
observed and simulated GPP.
```{r eval = evaluate}
# define model parameters
params_modl <- list(
    kphio           = 0.09423773,
    soilm_par_a     = 0.33349283
  )

# run the model for these parameters
output <- rsofun::runread_pmodel_f(
  p_model_drivers,
  par = params_modl
  )

# clean output for plotting
model_data <- output %>%
  filter(sitename == "FR-Pue") %>%
  tidyr::unnest(data) |>
  dplyr::slice(1:30)               # first month of data

# compute rmse
rmse <- sqrt(mean((validation_data$gpp - model_data$gpp)^2, na.rm = TRUE))

ggplot() +
    geom_line(
    data = model_data,
    aes(
      date,
      gpp
    ),
    colour = "red",
    alpha = 0.8
  ) +
  geom_line(
    data = validation_data,
    aes(
      date,
      gpp
    ),
    alpha = 0.8
  ) +
  geom_errorbar(
    data = validation_data,
    aes(x = date, ymin=gpp - 2*gpp_unc, ymax=gpp + 2*gpp_unc), width=.2,
                 position=position_dodge(0.05)) +
  geom_errorbar(
    data = model_data,
    aes(x = date, ymin=gpp - 2*rmse, ymax=gpp + 2*rmse), width=.2,
                 position=position_dodge(0.05), color = 'red') +
  labs(
    x = "Date",
    y = "GPP"
  )
```

Finally, we plot the GPP observation error (from Fluxnet) and simulation error (estimated). Following what the previous plots hinted, this shows that the observation error should not be used as standard deviation of the log-likelihood used in calibration, but that this standard deviation should also be a calibratable parameter and it's closer to the RMSE in magnitude.
```{r eval = evaluate}
params_new <- list(
  kphio = par_calib$par[1],
  soilm_par_a = 0.33349283
)

model_data_new <- rsofun::runread_pmodel_f(
  p_model_drivers,
  par = params_new
) |>
  dplyr::select(sitename, data) |>
  tidyr::unnest(data) |>
      dplyr::slice(1:110)

validation_data <- p_model_validation %>%
  tidyr::unnest(data) |>
  dplyr::slice(1:110)          # get first month of data

ggplot() +
    geom_line(
    data = model_data_new,
    aes(
      date,
      gpp
    ),
    colour = "red",
    alpha = 0.8
  ) +
  geom_line(
    data = validation_data,
    aes(
      date,
      gpp
    ),
    alpha = 0.8
  ) +
  geom_errorbar(
    data = validation_data,
    aes(x = date, ymin=gpp - 2*gpp_unc, ymax=gpp + 2*gpp_unc), width=.2,
                 position=position_dodge(0.05)) +
  geom_errorbar(
    data = model_data_new,
    aes(x = date, ymin=gpp - 2*par_calib$par[2],   # should cover ~ 95% of observations
        ymax=gpp + 2*par_calib$par[2]), width=.2,
                 position=position_dodge(0.05), 
    color = 'red', alpha = 0.5) +
  labs(
    x = "Date",
    y = "GPP"
  )
```
