---
title: "Sensitivity analysis"
author: "Pepa Aran"
date: "2023-04-18"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sensitivity analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(rsofun)
library(dplyr)
library(ggplot2)
library(tidyr)

library(sensitivity)
```

Parameter calibration can have a big impact on our modeling effort and use
big computational resources. Hence, it is worth our time to explore which parameters
should actually be calibrated (because of they impact the simulations greatly) 
and to examine if the calibration routines behave as expected. 
This vignette explains how to perform a simple
parameter sensitivity analysis for the P-model and how to interpret the outputs of the
calibration routines using the `BayesianTools` package.

## Morris sensitivity analysis

The Morris method for global sensitivity analysis allows to explore which parameters have the
biggest influence on the model fit. In this example, we will quantify how different
values of the calibratable parameters (`kphio`, `soilm_par_a`, `soilm_par_b`,
`tau_acclim_tempstress` and `par_shape_tempstress`) lead to more variability
in the match between GPP predicted by the P-model and GPP observations. It would
be wise to repeat this exercise for various targets because they may be simulated
by equations in the P-model that involve different model parameters.

If the P-model
has very low sensitivity to a certain parameter, calibrating this parameter will
not improve the model substantially. But if it's very sensitive to another parameter,
calibrating this second parameter could improve the P-model fit greatly. We should
spend our computational resources on calibrating the parameters to which the
model is most sensitive.

First of all, let's define a function which measures the agreement between
GPP predictions from the P-model and GPP observations, for a set of values of
the calibratable parameters. It computes the log-likelihood of 
the GPP predictions, given the observed GPP and its uncertainty.
We want to see how sensitive this function is to changes in the parameter values.

```{r}
# Define log-likelihood function
ll_pmodel <- function(
    par_v                   # a vector of calibratable parameters
){

  # get parameters into correct format
  par_list <- as.list(par_v)
  names(par_list) <- c(
    "kphio",
    "soilm_par_a",
    "soilm_par_b",
    "tau_acclim_tempstress",
    "par_shape_tempstress"
  )
  
  # simulate the model for the parameter list
  sim.df <- rsofun::runread_pmodel_f(
    drivers = rsofun::p_model_drivers,             # example forcing data
    par = par_list
  )
  
  # clean output
  sim.df <- sim.df |>
    dplyr::select(sitename, data) |>
    tidyr::unnest(data) |>                 
    dplyr::rename(gpp_mod = gpp)           # rename target column
  
  # get observations
  obs.df <- rsofun::p_model_validation |>  # example observed data
    dplyr::select(sitename, data) |>
    tidyr::unnest(data)
  
  # join observations and simulation output
  sim.df <- sim.df |>
    dplyr::left_join(
      obs.df, by = c("sitename", "date")
    ) |>
    tidyr::drop_na()     # remove NAs
  
  # calculate normal log-likelihood
  ll <- sum(
    stats::dnorm(
      sim.df[['gpp_mod']],                 # simulated GPP
      mean = sim.df[['gpp']],              # observed GPP
      sd = sim.df[['gpp_unc']],            # sd of observed GPP
      log = TRUE
    ))  
  
  if(is.nan(ll) | is.na(ll) | ll == 0 ){ ll <- - Inf}
  return(ll)
}

# Compute log-likelihood for a given set of parameters
ll_pmodel( par_v = c(
  kphio           = 0.09423773,
  soilm_par_a     = 0.33349283,
  soilm_par_b     = 1.45602286,
  tau_acclim_tempstress = 10,
  par_shape_tempstress  = 0.0
))
```

Next, let's define the parameter space by their lower and upper bounds. Some
parameters are constrained by their physical interpretation (e.g. `kphio > 0`)
and it's also necessary to provide a bounded parameter space for Morris'
method to sample the parameter space. 
```{r}
# best parameter values (from previous literature)
par_cal_best <- c(
    kphio           = 0.09423773,
    soilm_par_a     = 0.33349283,
    soilm_par_b     = 1.45602286,
    tau_acclim_tempstress = 10,
    par_shape_tempstress  = 0.0
  )

# lower bound
par_cal_min <- c(
    kphio           = 0,
    soilm_par_a     = 0,
    soilm_par_b     = 1,
    tau_acclim_tempstress = 0,
    par_shape_tempstress  = -0.5
  )

# upper bound
par_cal_max <- c(
    kphio           = 5,
    soilm_par_a     = 1,
    soilm_par_b     = 3,
    tau_acclim_tempstress = 20,
    par_shape_tempstress  = 0.5
  )
```

We use the `morris()` function from the `{sensitivity}` package to perform
the sensitivity analysis. As a target function, we will use the posterior
density (log-likelihood) of the parameters given the GPP data which we obtain
via the function `BayesianTools::createBayesianSetup()`. Note that, because of using
a uniform prior, the posterior distribution is proportional to the GPP 
log-likelihood (defined previously) 
wherever the parameter values are feasible and zero outside of the parameter ranges.
```{r}
morris_setup <- BayesianTools::createBayesianSetup(
  likelihood = ll_pmodel,
  prior = BayesianTools::createUniformPrior(par_cal_min, par_cal_max, par_cal_best),
  names = names(par_cal_best)
)
```

In the following chunk, we run the Morris sensitivity analysis, using a grid with
r=500 values for each parameter and a one-at-a-time design. Running the
sensitivity analysis may take a few minutes, even for this small example dataset,
and is still computationally cheaper than running the parameter calibration.
```{r}
set.seed(432)
morrisOut <- sensitivity::morris(
  model = morris_setup$posterior$density,
  factors = names(par_cal_best), 
  r = 500, 
  design = list(type = "oat", levels = 20, grid.jump = 3), 
  binf = par_cal_min, 
  bsup = par_cal_max, 
  scale = TRUE)
```

The analysis evaluates the variability of the target function, i.e. the 
log-likelihood, for several points across the parameter space. 
Statistics $\mu *$ and $\sigma$ measure the average of the absolute
differences 
```{r}
# summarise the moris output
morrisOut.df <- data.frame(
  parameter = names(par_cal_best),
  mu.star = apply(abs(morrisOut$ee), 2, mean, na.rm = T),
  sigma = apply(morrisOut$ee, 2, sd, na.rm = T)
) %>%
  arrange( mu.star )
```

Let's observe the results:
```{r}
morrisOut.df |>
  tidyr::gather(variable, value, -parameter) |>
  ggplot(aes(
    reorder(parameter, value),
    value, 
    fill = variable),
    color = NA) +
  geom_bar(position = position_dodge(), stat = 'identity') +
  scale_fill_brewer("", labels = c('mu.star' = expression(mu * "*"),
                                   'sigma' = expression(sigma)),
                    palette = "Dark2") +
  theme_classic() +
  theme(
    axis.text = element_text(size = 6),
    axis.title = element_blank(),
    legend.position = c(0.05, 0.95), legend.justification = c(0.05, 0.95)
  )

```


## Interpretation of calibration routine

It is always important to check the convergence of the MCMC algorithm used for the Bayesian calibration. We can plot the MCMC chains, returned as part of the `calib_sofun()` output, and see that they converge to a similar value and become progressively more stable. We can also see the probability density of the time series of parameter values. 

```{r}
plot(pars$mod)
```

For PEPA:
Let's take a look at the different sources of error in the model. First
we plot the observed GPP with the observation errors reported by FluxNet:
```{r}
validation_data <- p_model_validation %>%
  tidyr::unnest(data) |>
  dplyr::slice(1:30)          # get first month of data

ggplot() +
  #   geom_line(
  #   data = model_data,
  #   aes(
  #     date,
  #     gpp
  #   ),
  #   colour = "red",
  #   alpha = 0.8
  # ) +
  geom_line(
    data = validation_data,
    aes(
      date,
      gpp
    ),
    alpha = 0.8
  ) +
  geom_errorbar(
    data = validation_data,
    aes(x = date, ymin=gpp - 2*gpp_unc, ymax=gpp + 2*gpp_unc), width=.2,
                 position=position_dodge(0.05)) +
  labs(
    x = "Date",
    y = "GPP"
  )

```

Then, let's see what the average model error is, i.e. the RMSE between the 
observed and simulated GPP.
```{r}
# define model parameters
params_modl <- list(
    kphio           = 0.09423773,
    soilm_par_a     = 0.33349283,
    soilm_par_b     = 1.45602286,
    tau_acclim_tempstress = 10,
    par_shape_tempstress  = 0.0
  )

# run the model for these parameters
output <- rsofun::runread_pmodel_f(
  p_model_drivers,
  par = params_modl
  )

# clean output for plotting
model_data <- output %>%
  filter(sitename == "FR-Pue") %>%
  tidyr::unnest(data) |>
  dplyr::slice(1:30)               # first month of data

# compute rmse
rmse <- sqrt(mean((validation_data$gpp - model_data$gpp)^2, na.rm = TRUE))

ggplot() +
    geom_line(
    data = model_data,
    aes(
      date,
      gpp
    ),
    colour = "red",
    alpha = 0.8
  ) +
  geom_line(
    data = validation_data,
    aes(
      date,
      gpp
    ),
    alpha = 0.8
  ) +
  geom_errorbar(
    data = validation_data,
    aes(x = date, ymin=gpp - 2*gpp_unc, ymax=gpp + 2*gpp_unc), width=.2,
                 position=position_dodge(0.05)) +
  geom_errorbar(
    data = model_data,
    aes(x = date, ymin=gpp - 2*rmse, ymax=gpp + 2*rmse), width=.2,
                 position=position_dodge(0.05), color = 'red') +
  labs(
    x = "Date",
    y = "GPP"
  )
```
