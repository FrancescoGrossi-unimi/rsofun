---
title: "Parameter calibration and cost functions"
author: "Pepa Aran"
date: "2022-11-30"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Parameter calibration and cost functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(rsofun)
library(dplyr)
library(ggplot2)
```

The `rsofun` package allows to calibrate parameters of the `pmodel` and `biomee` models via the `calib_sofun()` function. The implementation of the calibration is fairly flexible and can be adapted to a specific use-case via a cost function (used as metrics for the optimization routines in `calib_sofun()`). The package provides a set of standard cost functions named `cost_*`, which can be used for a variety of calibrations (different sets of model parameters, using various target variables, etc.). Alternatively, it's possible to write a more specific new cost function to be used together with `calib_sofun()`.

In this vignette, we go over some examples on how to use the `rsofun` cost functions for parameter calibration and how to write your own custom one from scratch.

### Calibration to GPP using RMSE and GenSA optimizer

A simple approach to parameter calibration is to find the parameter values that lead to the best prediction performance, in terms of the RMSE (root mean squared error). The function `cost_rmse_pmodel()` runs the P-model internally to calculate the RMSE between predicted target values (in this case GPP) and the corresponding observations.

The implementation of `cost_rmse_pmodel()` allows flexibility in various ways. We can replicate the different calibration setups in Stocker et al., 2020 GMD. For example, following the `FULL` setup, all model parameters `kphio`, `soilm_par_a` and `soilm_par_b` are calibrated. Furthermore, the standard cost functions allow to calibrate to several targets (fluxes and leaf traits predicted by the P-model) simultaneously and to parallelize the simulations.

The syntax to run the calibration routine is as follows:
```{r}
# Define calibration settings and parameter ranges from previous work
settings_rmse <- list(
  method = 'GenSA',                   # minimizes the RMSE
  metric = cost_rmse_pmodel,          # our cost function
  control = list(                     # control parameters for the optimizer
    maxit = 10),
  par = list(                         # bounds for the parameter space
    kphio = list(lower=0.04, upper=0.2, init=0.05),
    soilm_par_a = list(lower=0.1, upper=5, init=2.4),
    soilm_par_b = list(lower=1, upper=2, init=1.5)
  )
)

# Calibrate the model and optimize the free parameters using
# demo datasets
pars_calib_rmse <- calib_sofun(
  # calib_sofun arguments:
  drivers = p_model_drivers,  
  obs = p_model_validation,
  settings = settings_rmse,
  # extra arguments passed to the cost function:
  setup = "FULL",           # calibrate all parameters
  targets = "gpp"           # define target variable GPP
)
```

The output of `calib_sofun()` is a list containing the calibrated parameter values and the raw optimization output from the optimizer (here from `GenSA` or, as we see next, from `BayesianTools::runMCMC`).

### Calibration to GPP using a simple likelihood function and BayesianTools

Let's calibrate only the `kphio` parameter (setup `'BRC'`), taking a Bayesian calibration approach. We assume that the target variable (`'gpp'`) follows a normal distribution centered at the observations and with its standard deviation being a new calibratable parameter (`'err_gpp'`). We also assume a uniform prior distribution for all calibratable parameters.
By maximizing the normal log-likelihood, the MAP (maximum a posteriori) estimators for `kphio` and `err_gpp` are computed.  With the function `cost_likelihood_pmodel()`, we can easily perform this calibration, as follows:

```{r eval = FALSE}
# Define calibration settings
settings_likelihood <- list(
  method = 'BayesianTools',
  metric = cost_likelihood_pmodel,                        # our cost function
  control = list(
    sampler = 'DEzs',
    settings = list(
      burnin = 5,
      iterations = 15
    )),
  par = list(
    kphio = list(lower=0.01, upper=0.2, init=0.05),    # uniform priors
    err_gpp = list(lower = 0.1, upper = 4, init = 2)   
  )
)

# Calibrate the model and optimize the free parameters using
# demo datasets
pars_calib_likelihood <- calib_sofun(
  # calib_sofun arguments:
  drivers = p_model_drivers,
  obs = p_model_validation,
  settings = settings_likelihood,
  # extra arguments passed ot the cost function:
  setup = "BRC",
  targets = "gpp",
  par_fixed = c(soilm_par_a = 0.3,    # fix parameter values from RMSE calibration
                soilm_par_b = 1)
)
```

Let's take a look at the code above. Since the P-model is run internally to make predictions, we must always specify which values the model parameters should take, i.e. the parameters that aren't calibrated `soilm_par_a` and `soilm_par_b` (via argument `par_fixed`).
In the definition of the calibration settings, the control parameters for `BayesianTools` differ from the ones used for `GenSA`. 

It is possible to calibrate the P-model to several targets (either fluxes or leaf traits) simultaneously using the cost functions presented here.  Furthermore, there are equivalent cost functions available for the BiomeE model. Check out the reference pages for more details on how to use `cost_likelihood_biomee()` and `cost_rmse_biomee()`.

### Write your custom cost function 

If the RMSE or normal log-likelihood (for one or several targets) cost functions that we provide do not fit your use case, you can easily write a custom one. In this section, we drive you through the main ideas with an example.

All cost functions must take at least three arguments:

* `par`: A vector of calibratable model parameters. In each iteration of the optimization, a new set of values of `par` is used to run the model and compute the cost.

* `obs`: A data frame of observations, against which to compare the simulation results.

* `drivers`: A data frame of driver data, used to run the simulations.

* Additional optional arguments can be used, like for example the model parameter values that should be fixed across simulations, etc.

Since we are calibrating the parameters based on model outputs, the cost function runs the P-model and compare its output to observed validation data.
```{r, eval = FALSE}
function(par, obs, drivers){
  # Your code
}
```

In the optimization procedure, the cost function only takes as argument the parameters `par` that are fed to `calib_sofun()` via `settings$par` (see previous sections). Nevertheless, within the cost function we call `runread_pmodel_f()` and this function needs a full set of model parameters. Therefore, the parameters that aren't being calibrated must be hard coded inside the cost function. In this example, we only want to calibrate the soil moisture stress parameters.
```{r, eval = FALSE}
function(par, obs, drivers){
  
  # Set values for the list of calibrated and non-calibrated model parameters
  params_modl <- list(
    kphio = 0.05,
    soilm_par_a = par[1],
    soilm_par_b = par[2]
  )
  
  # Run the model
  df <- runread_pmodel_f(
    drivers,
    par = params_modl,
    makecheck = TRUE,
    parallel = FALSE
  )
  
  # Your code to compute the cost
}
``` 

The following chunk defines the final function. We clean the observations and model output and align the data according to site and date, to compute the mean absolute error (MAE) on GPP. Finally, the function should return a scalar value, in this case the MAE, which we want to minimize. The GenSA optimization will minimize the cost, but if we wanted to use BayesianTools we should write `return(-cost)` because it's a maximizing routine. 
```{r}
cost_mae <- function(par, obs, drivers){

  # Set values for the list of calibrated and non-calibrated model parameters
  params_modl <- list(
    kphio = 0.05,
    soilm_par_a = par[1],
    soilm_par_b = par[2]
  )
  
  # Run the model
  df <- runread_pmodel_f(
    drivers = drivers,
    par = params_modl,
    makecheck = TRUE,
    parallel = FALSE
  )
  
  # Clean model output to compute cost
  df <- df %>%
    dplyr::select(sitename, data) %>%
    tidyr::unnest(data)
    
  # Clean validation data to compute cost
  obs <- obs %>%
    dplyr::select(sitename, data) %>%
    tidyr::unnest(data) %>%
    dplyr::rename('gpp_obs' = 'gpp') # rename for later
    
  # Left join model output with observations by site and date
  df <- dplyr::left_join(df, obs, by = c('sitename', 'date'))
  
  # Compute mean absolute error
  cost <- mean(abs(df$gpp - df$gpp_obs), na.rm = TRUE)
  
  # Return the computed cost
  return(cost)
}
``` 

As a last step, let's verify that the calibration procedure runs using this cost function.
```{r eval = FALSE}
# Define calibration settings and parameter ranges
settings_mae <- list(
  method = 'GenSA',
  metric = cost_mae, # our cost function
  control = list(
    maxit = 100),
  par = list(
    soilm_par_a = list(lower=0.1, upper=5, init=2.4),
    soilm_par_b = list(lower=1, upper=2, init=1.5)
  )
)

# Calibrate the model and optimize the free parameters
pars_calib_mae <- calib_sofun(
  drivers = p_model_drivers,
  obs = p_model_validation,
  settings = settings_mae
)
```